{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd=r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\ url_folder\\combined_urls_with_labels1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract character-based features\n",
    "def extract_char_features(url):\n",
    "    url = re.sub(r'^https?://', '', url)\n",
    "    char_list = list(url)\n",
    "    char_freq = Counter(char_list)\n",
    "    return dict(char_freq)\n",
    "\n",
    "# Function to extract n-gram features\n",
    "def extract_ngrams_features(url, n=3):\n",
    "    url = re.sub(r'^https?://', '', url)\n",
    "    ngrams_list = list(ngrams(url, n))\n",
    "    ngram_freq = Counter(ngrams_list)\n",
    "    return dict(ngram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the CSV file\n",
    "cd=r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\ url_folder\\combined_urls_with_labels1.csv\"\n",
    "df = pd.read_csv(cd)\n",
    "\n",
    "# Assuming the CSV file has columns 'url' and 'label'\n",
    "urls = df['url']\n",
    "labels = df['label']\n",
    "\n",
    "# Create lists to store the features and labels\n",
    "char_features_list = []\n",
    "ngram_features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process each URL and extract features\n",
    "for url, label in zip(urls, labels):\n",
    "    # Extract character-based features\n",
    "    char_features = extract_char_features(url)\n",
    "    char_features_list.append(char_features)\n",
    "    \n",
    "    # Extract n-gram features (adjust n as necessary)\n",
    "    ngram_features = extract_ngrams_features(url, n=3)\n",
    "    ngram_features_list.append(ngram_features)\n",
    "    \n",
    "    # Store the label\n",
    "    labels_list.append(label)\n",
    "\n",
    "# Flatten character features and n-gram features into separate columns\n",
    "char_df = pd.DataFrame(char_features_list)\n",
    "ngram_df = pd.DataFrame(ngram_features_list)\n",
    "\n",
    "# Ensure all DataFrames (character and n-gram features) have the same number of columns (use NaN for missing features)\n",
    "char_df = char_df.fillna(0).astype(int)\n",
    "ngram_df = ngram_df.fillna(0).astype(int)\n",
    "\n",
    "# Combine the original URL and label columns with the feature DataFrames\n",
    "feature_df = pd.concat([df[['url', 'label']], char_df, ngram_df], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_folder\\extracted_features.csv\"\n",
    "feature_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Features saved successfully to\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to the CSV file where the extracted features are stored\n",
    "input_csv = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_folder\\extracted_features.csv\"\n",
    "\n",
    "# Load the extracted features from the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Check the first few rows of the DataFrame to ensure correct loading\n",
    "print(\"Column Names:\", df.columns)\n",
    "print(df.head())\n",
    "\n",
    "# Extract the features and labels\n",
    "urls = df['url']\n",
    "labels = df['label']\n",
    "\n",
    "# For this case, features seem to be split into columns (like u, r, l, m, etc.)\n",
    "# Assuming columns starting from index 2 are feature columns (adjust as needed)\n",
    "feature_columns = df.columns[2:]\n",
    "\n",
    "# Create a feature matrix from these columns\n",
    "feature_matrix = df[feature_columns].values\n",
    "\n",
    "# Directory to save the images\n",
    "benign_folder = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_img\\benign_images\"\n",
    "non_benign_folder = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_img\\non_benign_images\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(benign_folder, exist_ok=True)\n",
    "os.makedirs(non_benign_folder, exist_ok=True)\n",
    "\n",
    "# Convert the features to vectors and reshape into images\n",
    "for idx, (features, label) in enumerate(zip(feature_matrix, labels)):\n",
    "    # Normalize the vector (min-max normalization to 0-255 range)\n",
    "    normalized_features = 255 * (features - np.min(features)) / (np.max(features) - np.min(features))\n",
    "    \n",
    "    # Ensure the vector length is 4096\n",
    "    if len(normalized_features) < 4096:\n",
    "        # Pad with zeros if the vector is too short\n",
    "        padded_features = np.pad(normalized_features, (0, 4096 - len(normalized_features)), mode='constant')\n",
    "    else:\n",
    "        # Truncate if the vector is too long\n",
    "        padded_features = normalized_features[:4096]\n",
    "    \n",
    "    # Reshape the 4096-length vector into a 64x64 image\n",
    "    reshaped_image = padded_features.reshape(64, 64)\n",
    "    \n",
    "    # Convert the 2D array into a grayscale image\n",
    "    img = Image.fromarray(reshaped_image.astype(np.uint8))\n",
    "\n",
    "    # Save the image to the appropriate folder based on the label\n",
    "    if label == 0:  # Benign\n",
    "        img.save(os.path.join(benign_folder, f\"benign_image_{idx}.png\"))\n",
    "    else:  # Non-benign\n",
    "        img.save(os.path.join(non_benign_folder, f\"non_benign_image_{idx}.png\"))\n",
    "\n",
    "print(\"Grayscale images have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "benign_folder = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_img\\benign_images\"\n",
    "non_benign_folder = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_img\\non_benign_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images_and_labels(benign_folder, non_benign_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load benign images\n",
    "    for filename in os.listdir(benign_folder):\n",
    "        if filename.endswith('.png'):\n",
    "            img = np.array(Image.open(os.path.join(benign_folder, filename)))\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Benign label\n",
    "    \n",
    "    # Load non-benign images\n",
    "    for filename in os.listdir(non_benign_folder):\n",
    "        if filename.endswith('.png'):\n",
    "            img = np.array(Image.open(os.path.join(non_benign_folder, filename)))\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Non-benign label\n",
    "\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_images_and_labels(benign_folder, non_benign_folder)\n",
    "\n",
    "# Preprocess data: Normalize and reshape for CNN input\n",
    "X = X.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
    "X = X.reshape(-1, 64, 64, 1)  # Reshape for CNN (64x64 pixels with 1 channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to binary format (not one-hot encoding)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # This will convert labels to [0, 1]\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for feature extraction\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(64, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the CNN model with validation data\n",
    "history = cnn_model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
    "                        validation_data=(X_test, y_test))  # Include validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "cnn_model.trainable = False  # Freeze the CNN\n",
    "train_features = cnn_model.predict(X_train)\n",
    "test_features = cnn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANN classifier\n",
    "ann_model = Sequential([\n",
    "    Dense(192, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the ANN model\n",
    "ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ANN classifier on features extracted by CNN with validation data\n",
    "history_ann = ann_model.fit(train_features, y_train, epochs=10, batch_size=32,\n",
    "                            validation_data=(test_features, y_test))  # Include validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ANN classifier on test data\n",
    "loss_ann, accuracy_ann = ann_model.evaluate(test_features, y_test)\n",
    "print(f'Test Accuracy: {accuracy_ann * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print validation accuracy and loss from training history of ANN\n",
    "print(\"Validation Loss:\", history_ann.history['val_loss'])\n",
    "print(\"Validation Accuracy:\", history_ann.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to extract character-based features\n",
    "def extract_char_features(url):\n",
    "    url = re.sub(r'^https?://', '', url)  # Remove protocol\n",
    "    char_list = list(url)\n",
    "    char_freq = Counter(char_list)\n",
    "    return dict(char_freq)\n",
    "\n",
    "# Function to extract n-gram features\n",
    "def extract_ngrams_features(url, n=3):\n",
    "    url = re.sub(r'^https?://', '', url)  # Remove protocol\n",
    "    return [''.join(gram) for gram in ngrams(url, n)]  # Return list of n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "cd = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_folder\\combined_urls_with_labels1.csv\"\n",
    "df = pd.read_csv(cd)\n",
    "\n",
    "# Assuming the CSV file has columns 'url' and 'label'\n",
    "urls = df['url']\n",
    "labels = df['label']\n",
    "\n",
    "# Extract character-based features\n",
    "char_features_list = []\n",
    "for url in urls:\n",
    "    char_features = extract_char_features(url)\n",
    "    char_features_list.append(char_features)\n",
    "\n",
    "# Convert character features to a DataFrame\n",
    "char_df = pd.DataFrame(char_features_list).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract n-gram strings for TF-IDF calculation\n",
    "n = 3  # Set n for n-grams\n",
    "ngram_strings = [' '.join(extract_ngrams_features(url, n)) for url in urls]\n",
    "\n",
    "# Calculate TF-IDF for n-grams\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(ngram_strings)\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Combine all features\n",
    "final_feature_df = pd.concat([df[['url', 'label']], char_df, tfidf_df], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_folder\\extracted_features_with_tfidf.csv\"\n",
    "final_feature_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Features with TF-IDF saved successfully to\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "csv_path = r\"C:\\Users\\HP\\Desktop\\jupyter project\\url_folder\\url_folder\\extracted_features_with_tfidf.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming the last column is the `label` and the rest are TF-IDF features\n",
    "tfidf_matrix = df.iloc[:, :-1].values  # All columns except the last one\n",
    "labels = df.iloc[:, -1].values  # The last column is the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming ngram_strings contains your n-gram features as strings\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(ngram_strings)  # This creates a sparse matrix\n",
    "tfidf_array = tfidf_matrix.toarray()  # Convert sparse matrix to dense array\n",
    "# If tfidf_matrix is already an array, just use it directly\n",
    "tfidf_array = tfidf_matrix  # No conversion needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming `tfidf_matrix` (from previous steps) and `labels` are already defined\n",
    "# Convert labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)  # 0 for benign, 1 for non-benign\n",
    "\n",
    "# Pad TF-IDF matrix to make it square\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "n_features = tfidf_array.shape[1]\n",
    "square_size = int(np.ceil(np.sqrt(n_features)))  # Find the nearest square size\n",
    "padded_array = np.pad(tfidf_array, ((0, 0), (0, square_size**2 - n_features)), 'constant')  # Pad with zeros\n",
    "reshaped_array = padded_array.reshape(-1, square_size, square_size, 1)  # Reshape for CNN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reshaped_array, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Build CNN model for feature extraction\n",
    "cnn_model_txt = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(square_size, square_size, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "])\n",
    "\n",
    "# Compile CNN\n",
    "cnn_model_txt.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN model\n",
    "cnn_model_txt.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using the CNN\n",
    "train_features = cnn_model_txt.predict(X_train)\n",
    "test_features = cnn_model_txt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build ANN Classifier\n",
    "ann_model_txt = Sequential([\n",
    "    Dense(192, activation='relu', input_dim=train_features.shape[1]),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile ANN\n",
    "ann_model_txt.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ANN classifier\n",
    "ann_model_txt.fit(train_features, y_train, epochs=10, batch_size=32, validation_data=(test_features, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Evaluate ANN on test data\n",
    "loss, accuracy = ann_model_txt.evaluate(test_features, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove the last classification layers from both CNN models\n",
    "text_feature_extractor = Model(inputs=cnn_model_txt.input, outputs=cnn_model_txt.layers[-2].output)\n",
    "image_feature_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train_text and X_train_images are your input data for text and images\n",
    "text_features_train = text_feature_extractor.predict(X_train)  # Features from text CNN\n",
    "text_features_test = text_feature_extractor.predict(X_test)    # Test features for text CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming X_train and X_test have shape (num_samples, 132, 132, 1)\n",
    "input_shape = (64, 64)  # Target input shape for the CNN model\n",
    "\n",
    "# Resize training and test images\n",
    "X_train_resized = tf.image.resize(X_train, input_shape)\n",
    "X_test_resized = tf.image.resize(X_test, input_shape)\n",
    "\n",
    "# Ensure the shape matches the expected input shape of the model\n",
    "X_train_resized = X_train_resized.numpy()  # Convert TensorFlow tensor to NumPy array if needed\n",
    "X_test_resized = X_test_resized.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_train = image_feature_extractor.predict(X_train_resized)\n",
    "image_features_test = image_feature_extractor.predict(X_test_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine features from text and image models\n",
    "train_features_combined = np.concatenate([text_features_train, image_features_train], axis=1)\n",
    "test_features_combined = np.concatenate([text_features_test, image_features_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ANN Classifier\n",
    "hf_ann_model = Sequential([\n",
    "    Dense(192, activation=\"relu\", input_dim=train_features_combined.shape[1]),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\"),  # Binary classification\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ANN Model\n",
    "hf_ann_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train ANN Classifier\n",
    "hf_ann_model.fit(train_features_combined, y_train, validation_data=(test_features_combined, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate ANN\n",
    "loss, accuracy = hf_ann_model.evaluate(test_features_combined, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data using combined features\n",
    "y_pred_prob = hf_ann_model.predict(test_features_combined)  # Use combined features\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions (0 or 1)\n",
    "\n",
    "# Ensure true labels are correctly formatted\n",
    "y_true = y_test.flatten()  # Flatten y_test if necessary (if it's one-dimensional already, this does nothing)\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Benign', 'Non-Benign'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
